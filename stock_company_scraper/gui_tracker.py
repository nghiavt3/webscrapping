import sqlite3
import tkinter as tk
import sys
import hashlib
import time
import webbrowser
import re
from tkinter import ttk, messagebox
from datetime import datetime, date, timedelta
import os
import threading
import subprocess
import requests
import fitz
from io import BytesIO
from concurrent.futures import ThreadPoolExecutor, as_completed
import winsound 
from urllib.parse import unquote, quote
# --- THÆ¯ VIá»†N Bá»” SUNG ---
try:
    import google.generativeai as genai
except ImportError:
    pass

# --- 1. KIá»‚M TRA QUYá»€N TRUY Cáº¬P ---
def check_access():
    if len(sys.argv) < 2:
        sys.exit("Truy cáº­p bá»‹ cháº·n! Vui lÃ²ng khá»Ÿi Ä‘á»™ng tá»« App Web.")

    received_token = sys.argv[1]
    valid_tokens = []
    for offset in [0, -1]:
        t_str = (datetime.now() + timedelta(minutes=offset)).strftime('%Y-%m-%d %H:%M')
        raw = f"MySecretKey_{t_str}"
        valid_tokens.append(hashlib.sha256(raw.encode()).hexdigest())

    if received_token not in valid_tokens:
        root_auth = tk.Tk()
        root_auth.withdraw()
        messagebox.showerror("Lá»—i báº£o máº­t", "Token háº¿t háº¡n hoáº·c khÃ´ng há»£p lá»‡!")
        root_auth.destroy()
        sys.exit()

check_access()

# --- 2. Cáº¤U HÃŒNH Dá»® LIá»†U & AI ---
try:
    from spider_names import ALL_SPIDERS
    ALL_SPIDERS = sorted(ALL_SPIDERS) 
except ImportError:
    ALL_SPIDERS = [] 

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATABASE_NAME = os.path.join(BASE_DIR, 'stock_events.db')
current_view_data = [] 
item_map = {}
last_count = 0 

# Cáº¤U HÃŒNH AI CHUáº¨N (ÄÃ£ sá»­a lá»—i 404)
try:
    genai.configure(api_key="AIzaSyCaGKXaOKGFRq73Qh-psbglhTkCkxpkpPw")
    AI_MODEL = genai.GenerativeModel('gemini-flash-latest')
    print("AI Ä‘Ã£ sáºµn sÃ ng.")
except Exception as e:
    print(f"Lá»—i khá»Ÿi táº¡o AI: {e}")

# --- 3. LOGIC Xá»¬ LÃ URL, AI & HIá»‚N THá»Š ---

def init_ai_cache_table():
    conn = sqlite3.connect(DATABASE_NAME)
    cursor = conn.cursor()
    # Táº¡o báº£ng lÆ°u trá»¯ AI náº¿u chÆ°a cÃ³
    # pdf_url lÃ  PRIMARY KEY Ä‘á»ƒ Ä‘áº£m báº£o khÃ´ng lÆ°u trÃ¹ng
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS ai_cache (
            pdf_url TEXT PRIMARY KEY,
            mcp TEXT,
            analysis_result TEXT,
            sentiment_score INTEGER,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    try:
        #cursor.execute("ALTER TABLE ai_cache ADD COLUMN mcp TEXT")
        cursor.execute("ALTER TABLE ai_cache ADD COLUMN sentiment_score INTEGER")
        print("thÃªm column sentiment_score.")
    except sqlite3.OperationalError:
        pass # Cá»™t Ä‘Ã£ tá»“n táº¡i
    conn.commit()
    conn.close()

init_ai_cache_table()
def get_cached_ai(pdf_url):
    """Láº¥y káº¿t quáº£ tá»« báº£ng cache dá»±a trÃªn link PDF"""
    try:
        conn = sqlite3.connect(DATABASE_NAME)
        cursor = conn.cursor()
        cursor.execute("SELECT analysis_result FROM ai_cache WHERE pdf_url = ?", (pdf_url,))
        row = cursor.fetchone()
        conn.close()
        return row[0] if row else None
    except: return None

def save_ai_to_cache(pdf_url,mcp,score, result_text):
    """LÆ°u káº¿t quáº£ phÃ¢n tÃ­ch má»›i vÃ o báº£ng cache"""
    try:
        conn = sqlite3.connect(DATABASE_NAME)
        cursor = conn.cursor()
        cursor.execute("INSERT OR REPLACE INTO ai_cache (pdf_url,mcp,sentiment_score, analysis_result) VALUES (?, ?, ?, ?)", 
                       (pdf_url,mcp,score, result_text))
        conn.commit()
        conn.close()
    except Exception as e:
        print(f"Lá»—i lÆ°u cache: {e}")

def open_url(event):
    try:
        tags = event.widget.tag_names(tk.CURRENT)
        for tag in tags:
            if tag.startswith("http"):
                webbrowser.open(tag)
                return
    except Exception as e:
        print(f"Lá»—i má»Ÿ link: {e}")

def highlight_urls(text_widget):
    content = text_widget.get("1.0", tk.END)
    # Regex má»›i: Báº¯t Ä‘áº§u báº±ng http vÃ  láº¥y táº¥t cáº£ kÃ½ tá»± cho Ä‘áº¿n khi gáº·p xuá»‘ng dÃ²ng hoáº·c dáº¥u ngoáº·c kÃ©p
    url_pattern = r'(https?://[^\s"\'\n]+(?:%20|[^\s"\'\n])*|https?://[^\n"\'<>]+)'
    
    for tag in text_widget.tag_names():
        if tag.startswith("http"): 
            text_widget.tag_delete(tag)
            
    for match in re.finditer(url_pattern, content):
        start = f"1.0 + {match.start()} chars"
        end = f"1.0 + {match.end()} chars"
        url = match.group(0).strip() # XÃ³a khoáº£ng tráº¯ng thá»«a á»Ÿ Ä‘áº§u/cuá»‘i náº¿u cÃ³
        
        # QUAN TRá»ŒNG: MÃ£ hÃ³a link Ä‘á»ƒ Ä‘áº£m báº£o Ctrl+Click luÃ´n Ä‘Ãºng
        from urllib.parse import quote, unquote
        # Giáº£i mÃ£ trÆ°á»›c rá»“i mÃ£ hÃ³a láº¡i Ä‘á»ƒ trÃ¡nh double-encoding
        safe_url = quote(unquote(url), safe=':/?&=#+')
        
        text_widget.tag_add(safe_url, start, end)
        text_widget.tag_config(safe_url, foreground="#0066CC", underline=True)
        text_widget.tag_bind(safe_url, "<Control-Button-1>", open_url)

def analyze_pdf_with_ai(pdf_url):
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        # Táº¡o session Ä‘á»ƒ giá»¯ cookie (quan trá»ng vá»›i Google Drive)
        session = requests.Session()
        response = session.get(pdf_url, headers=headers, timeout=45, verify=False)
        # Xá»­ lÃ½ xÃ¡c nháº­n virus cá»§a Google Drive (náº¿u file hÆ¡i náº·ng)
        if 'confirm=' in response.text and 'drive.google.com' in pdf_url:
            confirm_match = re.search(r'confirm=([a-zA-Z0-9_-]+)', response.text)
            if confirm_match:
                confirm_token = confirm_match.group(1)
                pdf_url = pdf_url + "&confirm=" + confirm_token
                response = session.get(pdf_url, headers=headers, timeout=45, verify=False)
        if response.status_code != 200:
            return f"âŒ Lá»—i táº£i file: HTTP {response.status_code}"
        
        pdf_blob = response.content
        
        if b"%PDF" not in pdf_blob[:1024]: # Kiá»ƒm tra trong 1KB Ä‘áº§u tiÃªn
            # Debug: In ra 100 kÃ½ tá»± Ä‘áº§u Ä‘á»ƒ xem nÃ³ lÃ  gÃ¬ (cÃ³ thá»ƒ lÃ  HTML lá»—i)
            print(f"Ná»™i dung lá»—i: {pdf_blob[:100]}")
            return "âŒ Lá»—i: Link Google Drive nÃ y yÃªu cáº§u quyá»n truy cáº­p hoáº·c khÃ´ng cho phÃ©p táº£i trá»±c tiáº¿p."
        prompt = """
        HÃ£y phÃ¢n tÃ­ch file PDF Ä‘Ã­nh kÃ¨m (cÃ³ thá»ƒ lÃ  vÄƒn báº£n scan):
        1. TÃ³m táº¯t 3 ná»™i dung quan trá»ng nháº¥t áº£nh hÆ°á»Ÿng Ä‘áº¿n doanh nghiá»‡p.
        2. ÄÃ¡nh giÃ¡ tÃ¡c Ä‘á»™ng Ä‘áº¿n giÃ¡ cá»• phiáº¿u: TÃ­ch cá»±c, TiÃªu cá»±c hay Trung tÃ­nh?
        3. Cháº¥m Ä‘iá»ƒm má»©c Ä‘á»™ áº£nh hÆ°á»Ÿng: Tá»« -10 (Ráº¥t xáº¥u) Ä‘áº¿n +10 (Ráº¥t tá»‘t).
        4. So sÃ¡nh vá»›i dá»¯ liá»‡u cÃ¹ng ká»³ vÃ  Ä‘á»‹nh giÃ¡ theo p/b ,p/e
        YÃªu cáº§u tráº£ lá»i báº±ng tiáº¿ng Viá»‡t, ngáº¯n gá»n vÃ  trá»±c diá»‡n.
        """
        response_ai = AI_MODEL.generate_content([
            prompt,
            {"mime_type": "application/pdf", "data": pdf_blob}
        ])
        return response_ai.text
    except Exception as e:
        return f"âŒ Lá»—i AI: {str(e)}"
def clean_pdf_url(raw_url):
    """Chuyá»ƒn Ä‘á»•i link Google Drive sang link stream trá»±c tiáº¿p"""
    # 1. LÃ m sáº¡ch sÆ¡ bá»™
    raw_url = unquote(raw_url.strip())
    
    # 2. Xá»­ lÃ½ link Google Drive
    if 'drive.google.com' in raw_url:
        file_id_match = re.search(r'/d/([a-zA-Z0-9_-]+)', raw_url)
        if file_id_match:
            file_id = file_id_match.group(1)
            # Sá»­ dá»¥ng link export hoáº·c uc
            return f"https://drive.google.com/uc?export=download&id={file_id}"

    # 3. Xá»­ lÃ½ link tá»« redirect (náº¿u cÃ³)
    n = raw_url.find('url=')
    if n != -1:
        raw_url = unquote(raw_url[n+4:])
    
    return raw_url

def trigger_ai_analysis():
    selected = tree.focus()
    if not selected:
        messagebox.showwarning("ChÃº Ã½", "Vui lÃ²ng chá»n má»™t tin trÃªn báº£ng!")
        return
    raw_url = None # Khá»Ÿi táº¡o giÃ¡ trá»‹ máº·c Ä‘á»‹nh
    try:
        # Kiá»ƒm tra xem ngÆ°á»i dÃ¹ng cÃ³ Ä‘ang bÃ´i Ä‘en (selection) vÄƒn báº£n khÃ´ng
        raw_url = detail_box.get(tk.SEL_FIRST, tk.SEL_LAST).strip()
    except tk.TclError:
        pass # KhÃ´ng cÃ³ vÃ¹ng chá»n
    # 2. Náº¿u khÃ´ng bÃ´i Ä‘en, tá»± Ä‘á»™ng tÃ¬m link trong ná»™i dung báº£n tin
    if not raw_url:
        item_data = tree.item(selected)
        #ticker = item_data['values'][1] # Láº¥y mÃ£ CP
        content = item_map.get(selected, "")
        pdf_match = re.search(r'(https?://[^\s<>"]+)', content)
        if pdf_match:
            raw_url = pdf_match.group(0)

    # 3. Kiá»ƒm tra xem cuá»‘i cÃ¹ng cÃ³ láº¥y Ä‘Æ°á»£c link nÃ o khÃ´ng
    if not raw_url:
        messagebox.showinfo("ThÃ´ng tin", "Vui lÃ²ng bÃ´i Ä‘en Ä‘Æ°á»ng link cá»¥ thá»ƒ hoáº·c chá»n tin cÃ³ liÃªn káº¿t!")
        return
    

    # Sá»¬ Dá»¤NG HÃ€M LÃ€M Sáº CH LINK á» ÄÃ‚Y
    pdf_url = clean_pdf_url(raw_url) 
    
    is_google_drive = "drive.google.com" in pdf_url
    #is_direct_pdf = pdf_url.lower().split('?')[0].endswith('.pdf')
    is_direct_pdf =".pdf" in pdf_url.lower()
    is_export = "uc?export=" in pdf_url.lower()
    
    if not (is_google_drive or is_direct_pdf or is_export):
        messagebox.showinfo("ThÃ´ng tin", "Link nÃ y khÃ´ng nháº­n diá»‡n Ä‘Æ°á»£c Ä‘á»‹nh dáº¡ng PDF.")
        return
    ticker = tree.item(selected)['values'][1]
    # --- BÆ¯á»šC KIá»‚M TRA CACHE Táº I Báº¢NG RIÃŠNG ---
    cached_result = get_cached_ai(pdf_url)
    if cached_result:
        print(f"ğŸš€ TÃ¬m tháº¥y cache cho PDF: {pdf_url}")
        display_ai_popup(ticker, f"[Káº¾T QUáº¢ ÄÃƒ LÆ¯U TRÆ¯á»šC ÄÃ“]\n\n{cached_result}", pdf_url)
        return
    # Hiá»ƒn thá»‹ thÃ´ng bÃ¡o Ä‘ang xá»­ lÃ½ trÃªn giao diá»‡n chÃ­nh
    detail_box.config(state=tk.NORMAL)
    detail_box.insert(tk.END, f"\n\nğŸ¤– ÄANG PHÃ‚N TÃCH AI CHO MÃƒ {ticker}... Vui lÃ²ng Ä‘á»£i cá»­a sá»• má»›i.")
    detail_box.see(tk.END)
    detail_box.config(state=tk.DISABLED)
    
    def worker():
        result = analyze_pdf_with_ai(pdf_url)
        if "âŒ" not in result:
            # TrÃ­ch xuáº¥t Ä‘iá»ƒm sá»‘ tá»« ná»™i dung AI tráº£ vá» (Ä‘á»ƒ lÆ°u vÃ o cá»™t riÃªng náº¿u cáº§n)
            score_match = re.search(r"(-?\d+)", result)
            score = int(score_match.group(1)) if score_match else 0
            save_ai_to_cache(pdf_url,ticker,score, result) # LÆ°u vÃ o báº£ng ai_cache
        root.after(0, lambda: display_ai_popup(ticker, result, pdf_url))

    threading.Thread(target=worker, daemon=True).start()

def display_ai_popup(ticker, result, url):
    """Táº¡o má»™t cá»­a sá»• Popup má»›i Ä‘á»ƒ hiá»ƒn thá»‹ káº¿t quáº£ AI"""
    popup = tk.Toplevel(root)
    popup.title(f"AI Analyst - MÃ£: {ticker}")
    popup.geometry("700x550")
    popup.configure(bg="#F0F2F5")

    # Header
    header = tk.Label(popup, text=f"BÃO CÃO PHÃ‚N TÃCH AI - {ticker}", 
                      font=('Segoe UI', 14, 'bold'), bg="#F0F2F5", fg="#1A73E8")
    header.pack(pady=10)

    # Text Area
    text_frame = ttk.Frame(popup)
    text_frame.pack(fill='both', expand=True, padx=20, pady=5)
    
    ai_box = tk.Text(text_frame, wrap=tk.WORD, font=('Segoe UI', 11), 
                     padx=15, pady=15, bg="white", borderwidth=0)
    ai_box.pack(side='left', fill='both', expand=True)
    
    scrollbar = ttk.Scrollbar(text_frame, command=ai_box.yview)
    scrollbar.pack(side='right', fill='y')
    ai_box.config(yscrollcommand=scrollbar.set)

    # ChÃ¨n dá»¯ liá»‡u
    ai_box.insert(tk.END, f"Nguá»“n file: {url}\n")
    ai_box.insert(tk.END, "-"*50 + "\n\n")
    ai_box.insert(tk.END, result)
    
    highlight_urls(ai_box)
    ai_box.config(state=tk.DISABLED)

    # Footer
    btn_close = ttk.Button(popup, text="ÄÃ³ng", command=popup.destroy)
    btn_close.pack(pady=10)

def get_text_from_pdf_url(pdf_url):
    """Táº£i file PDF tá»« URL vÃ  trÃ­ch xuáº¥t vÄƒn báº£n"""
    try:
        response = requests.get(pdf_url, timeout=15)
        if response.status_code == 200:
            with fitz.open(stream=BytesIO(response.content), filetype="pdf") as doc:
                text = ""
                # Chá»‰ láº¥y 3-5 trang Ä‘áº§u Ä‘á»ƒ trÃ¡nh quÃ¡ táº£i AI vÃ  tiáº¿t kiá»‡m token
                for page in doc[:5]:
                    text += page.get_text()
                return text
        return None
    except Exception as e:
        print(f"Lá»—i Ä‘á»c PDF: {e}")
        return None
def analyze_market_impact(pdf_url, summary_fallback):
    """Logic phÃ¢n tÃ­ch: Check Cache -> Read PDF -> AI Analyze -> Save Cache"""
    cached_result = get_cached_ai(pdf_url)
    
    if cached_result:
        print(f"ğŸš€ TÃ¬m tháº¥y cache cho PDF: {pdf_url}")
        #display_ai_popup('LUATVIETNAM', f"[Káº¾T QUáº¢ ÄÃƒ LÆ¯U TRÆ¯á»šC ÄÃ“]\n\n{cached_result}", pdf_url)
        return f"[Dá»® LIá»†U Tá»ª CACHE]\n{cached_result}"

    # 2. Náº¿u chÆ°a cÃ³, tiáº¿n hÃ nh Ä‘á»c ná»™i dung PDF
    pdf_text = get_text_from_pdf_url(pdf_url)
    
    # Náº¿u khÃ´ng Ä‘á»c Ä‘Æ°á»£c PDF, dÃ¹ng tiÃªu Ä‘á» (summary) Ä‘á»ƒ phÃ¢n tÃ­ch táº¡m thá»i
    input_content = pdf_text if pdf_text and len(pdf_text) > 100 else summary_fallback

    prompt = f"""
    Báº¡n lÃ  chuyÃªn gia phÃ¢n tÃ­ch chÃ­nh sÃ¡ch kinh táº¿. HÃ£y Ä‘á»c ná»™i dung vÄƒn báº£n sau vÃ  Ä‘Ã¡nh giÃ¡ tÃ¡c Ä‘á»™ng Ä‘áº¿n TTCK Viá»‡t Nam:
    Ná»™i dung: {input_content[:4000]} 
    
    YÃªu cáº§u xuáº¥t ra Ä‘á»‹nh dáº¡ng sau:
    - TÃ“M Táº®T: (1-2 cÃ¢u chÃ­nh yáº¿u)
    - NHÃ“M NGÃ€NH HÆ¯á»NG Lá»¢I:
    - NHÃ“M NGÃ€NH Rá»¦I RO:
    - ÄIá»‚M TÃC Äá»˜NG: (Tá»« -10 Ä‘áº¿n +10)
    - CHIáº¾N LÆ¯á»¢C: (Mua/BÃ¡n/Theo dÃµi)
    """

    try:
        response = AI_MODEL.generate_content(prompt)
        analysis_text = response.text
        
        # TrÃ­ch xuáº¥t Ä‘iá»ƒm sá»‘ tá»« ná»™i dung AI tráº£ vá» (Ä‘á»ƒ lÆ°u vÃ o cá»™t riÃªng náº¿u cáº§n)
        score_match = re.search(r"(-?\d+)", analysis_text)
        score = int(score_match.group(1)) if score_match else 0

        
        # 3. LÆ°u vÃ o Cache
        save_ai_to_cache(pdf_url,'LUATVIETNAM',score, analysis_text)
        return analysis_text
    except Exception as e:
        return f"âš ï¸ Lá»—i AI: {str(e)}"
def run_auto_impact_assessment():
    selected = tree.focus()
    
    if not selected:
        messagebox.showinfo("ChÃº Ã½", "Vui lÃ²ng chá»n má»™t dÃ²ng dá»¯ liá»‡u.")
        return
    raw_url = None # Khá»Ÿi táº¡o giÃ¡ trá»‹ máº·c Ä‘á»‹nh
    try:
        # Kiá»ƒm tra xem ngÆ°á»i dÃ¹ng cÃ³ Ä‘ang bÃ´i Ä‘en (selection) vÄƒn báº£n khÃ´ng
        raw_url = detail_box.get(tk.SEL_FIRST, tk.SEL_LAST).strip()
    except tk.TclError:
        pass # KhÃ´ng cÃ³ vÃ¹ng chá»n
    if not raw_url:
        item_data = tree.item(selected)
        #ticker = item_data['values'][1] # Láº¥y mÃ£ CP
        content = item_map.get(selected, "")
        pdf_match = re.search(r'(https?://[^\s<>"]+)', content)
        if pdf_match:
            raw_url = pdf_match.group(0)

        # 3. Kiá»ƒm tra xem cuá»‘i cÃ¹ng cÃ³ láº¥y Ä‘Æ°á»£c link nÃ o khÃ´ng
    if not raw_url:
        messagebox.showinfo("ThÃ´ng tin", "Vui lÃ²ng bÃ´i Ä‘en Ä‘Æ°á»ng link cá»¥ thá»ƒ hoáº·c chá»n tin cÃ³ liÃªn káº¿t!")
        return
    
    # Giáº£ Ä‘á»‹nh cá»™t chá»©a URL download lÃ  cá»™t cuá»‘i cÃ¹ng hoáº·c dá»±a trÃªn logic scraper cá»§a báº¡n
    # Báº¡n cáº§n Ä‘áº£m báº£o details_raw chá»©a URL PDF
    summary = item_data['values'][3]
    ticker = item_data['values'][1]
    
    if ticker != "LUATVIETNAM":
        messagebox.showwarning("ChÃº Ã½", "Chá»©c nÄƒng nÃ y tá»‘i Æ°u cho dá»¯ liá»‡u vÄƒn báº£n phÃ¡p luáº­t.")
        return
    # TÃ¬m link PDF trong chuá»—i details_raw báº±ng Regex
    #pdf_links = re.findall(r'(https?://[^\s]', raw_url)
    pdf_url = raw_url if raw_url else None

    if not pdf_url:
        messagebox.showwarning("Thiáº¿u dá»¯ liá»‡u", "KhÃ´ng tÃ¬m tháº¥y link PDF Ä‘á»ƒ phÃ¢n tÃ­ch chuyÃªn sÃ¢u.")
        return

    # Hiá»ƒn thá»‹ cá»­a sá»• káº¿t quáº£
    popup = tk.Toplevel(root)
    popup.title("PhÃ¢n tÃ­ch tÃ¡c Ä‘á»™ng AI (Deep Analysis)")
    popup.geometry("600x500")
    
    txt = tk.Text(popup, wrap=tk.WORD, font=('Segoe UI', 10), padx=15, pady=15)
    txt.pack(fill='both', expand=True)
    txt.insert(tk.END, "ğŸš€ Äang Ä‘á»c PDF vÃ  phÃ¢n tÃ­ch chuyÃªn sÃ¢u... Vui lÃ²ng Ä‘á»£i...")

    def worker():
        res = analyze_market_impact(pdf_url, summary)
        root.after(0, lambda: txt.delete('1.0', tk.END))
        root.after(0, lambda: txt.insert(tk.END, res))

    threading.Thread(target=worker, daemon=True).start()
def show_luatvietnam_only():
    """Chá»©c nÄƒng nÃºt má»›i: Chá»‰ hiá»ƒn thá»‹ dá»¯ liá»‡u tá»« báº£ng event_luatvietnam"""
    global current_view_data
    fetch_history_data("event_luatvietnam")
    root.title("Stock Scraper - ChuyÃªn má»¥c VÄƒn báº£n PhÃ¡p luáº­t (LuatVietNam)")
    
# --- 4. TRUY Váº¤N Dá»® LIá»†U ---

def fetch_history_data(table_name):
    global current_view_data
    if not table_name: return
    conn = sqlite3.connect(DATABASE_NAME)
    cursor = conn.cursor()
    try:
        cursor.execute(f"""SELECT t.id, t.mcp, t.date, t.summary, t.scraped_at, t.web_source, t.details_clean, c.sentiment_score 
                       FROM {table_name} t LEFT JOIN ai_cache c ON t.details_clean LIKE '%' || c.pdf_url || '%'
                        ORDER BY t.date DESC
                       """)
        rows = cursor.fetchall()
        processed_data = []
        for row in rows:
            new_row = list(row)
            if not new_row[2] or new_row[2] == "None":
                new_row[2] = row[4].split(' ')[0] if row[4] else "N/A"
                
            #processed_data.append(tuple(new_row))
            # LÆ°u cáº£ dá»¯ liá»‡u vÃ  tag vÃ o list (dÃ¹ng tuple Ä‘á»ƒ quáº£n lÃ½)
            processed_data.append(tuple(new_row))
            
        # Sáº¯p xáº¿p theo ngÃ y
        current_view_data = sorted(processed_data, key=lambda x: str(x[2]), reverse=True)
        update_treeview(tree, current_view_data)
        root.title(f"Stock Scraper - {table_name} ({len(current_view_data)} báº£n ghi)")
    except Exception as e:
        messagebox.showerror("Lá»—i", f"KhÃ´ng thá»ƒ Ä‘á»c báº£ng {table_name}: {e}")
    finally:
        conn.close()

def get_filtered_data(days_offset=None):
    today_dt = date.today()
    limit_date = today_dt - timedelta(days=days_offset-1) if days_offset else today_dt
    all_data = []
    if not os.path.exists(DATABASE_NAME): return []
    conn = sqlite3.connect(DATABASE_NAME)
    cursor = conn.cursor()
    try:
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name LIKE 'event_%'")
        tables = [row[0] for row in cursor.fetchall()]
        for table in tables:
            # THÃŠM LEFT JOIN VÃ€O ÄÃ‚Y
            query = f"""
                SELECT t.id, t.mcp, t.date, t.summary, t.scraped_at, t.web_source, t.details_clean, c.sentiment_score 
                FROM {table} t 
                LEFT JOIN ai_cache c ON t.details_clean LIKE '%' || c.pdf_url || '%'
            """
            cursor.execute(query)
            for row in cursor.fetchall():
                raw_date_str = row[2] if row[2] and row[2] != "None" else row[4]
                if raw_date_str:
                    try:
                        clean_date_str = raw_date_str.split(' ')[0]
                        record_date = datetime.strptime(clean_date_str, '%Y-%m-%d').date()
                        if record_date >= limit_date:
                            new_row = list(row)
                            new_row[2] = clean_date_str
                            all_data.append(tuple(new_row))
                    except: continue
    finally: conn.close()
    return sorted(all_data, key=lambda x: str(x[2]), reverse=True)

def get_newly_scraped_data():
    today_str = date.today().strftime('%Y-%m-%d')
    all_data = []
    if not os.path.exists(DATABASE_NAME): return []
    conn = sqlite3.connect(DATABASE_NAME)
    cursor = conn.cursor()
    try:
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name LIKE 'event_%'")
        tables = [row[0] for row in cursor.fetchall()]
        for table in tables:
            # THÃŠM LEFT JOIN VÃ€O ÄÃ‚Y
            query = f"""
                SELECT t.id, t.mcp, t.date, t.summary, t.scraped_at, t.web_source, t.details_clean, c.sentiment_score 
                FROM {table} t 
                LEFT JOIN ai_cache c ON t.details_clean LIKE '%' || c.pdf_url || '%'
                WHERE t.scraped_at LIKE ?
            """
            cursor.execute(query, (f"{today_str}%",))
            for row in cursor.fetchall():
                new_row = list(row)
                if not new_row[2] or new_row[2] == "None":
                    new_row[2] = row[4].split(' ')[0]
                all_data.append(tuple(new_row))
    finally: conn.close()
    return sorted(all_data, key=lambda x: x[4], reverse=True)

def perform_search():
    query = search_var.get().strip().upper()
    if not query:
        # Náº¿u Ä‘á»ƒ trá»‘ng Ã´ tÃ¬m kiáº¿m, hiá»ƒn thá»‹ láº¡i toÃ n bá»™ dá»¯ liá»‡u hiá»‡n táº¡i
        update_treeview(tree, current_view_data)
        return
    
    # Lá»c dá»±a trÃªn cá»™t MÃ£ CP (index 1) hoáº·c ná»™i dung TÃ³m táº¯t (index 3)
    filtered = [
        row for row in current_view_data 
        if query in str(row[1]).upper() or query in str(row[3]).upper()
    ]
    
    update_treeview(tree, filtered)
    
    # Cáº­p nháº­t tiÃªu Ä‘á» Ä‘á»ƒ biáº¿t Ä‘ang xem káº¿t quáº£ tÃ¬m kiáº¿m
    root.title(f"Káº¿t quáº£ tÃ¬m kiáº¿m cho: {query} ({len(filtered)} báº£n ghi)")

def run_parallel_logic(progress_bar, run_btn):
    total = len(ALL_SPIDERS)
    if total == 0: return
    try: max_parallel = int(worker_combo.get())
    except: max_parallel = 3
    completed = 0
    with ThreadPoolExecutor(max_workers=max_parallel) as executor:
        futures = {executor.submit(lambda s: subprocess.run(['scrapy', 'crawl', s], shell=True, cwd=BASE_DIR), s): s for s in ALL_SPIDERS}
        for future in as_completed(futures):
            completed += 1
            root.after(0, lambda p=(completed/total)*100: progress_bar.config(value=p))
            root.after(0, lambda c=completed, t=total: run_btn.config(text=f"â³ ({c}/{t})..."))
    root.after(0, lambda: finalize_run(run_btn))

def finalize_run(run_btn):
    run_btn.config(state=tk.NORMAL, text="ğŸš€ Cháº¡y Scrapers")
    messagebox.showinfo("Xong", "ÄÃ£ cáº­p nháº­t dá»¯ liá»‡u má»›i!")
    update_display("today")

def auto_refresh():
    global last_count
    data_today = get_filtered_data(days_offset=1)
    current_count = len(data_today)
    if current_count > last_count and last_count != 0:
        winsound.Beep(1000, 500)
        update_display("today")
    last_count = current_count
    root.after(300000, auto_refresh)

# --- 5. GIAO DIá»†N CHÃNH ---

def update_treeview(tree_widget, data):
    global item_map
    item_map = {} # Reset báº£n Ä‘á»“ dá»¯ liá»‡u
    today_str = date.today().strftime('%Y-%m-%d')
    
    for item in tree_widget.get_children(): 
        tree_widget.delete(item)
    
    for row in data:
        # row: (id, mcp, date, summary, scraped_at, source, details_clean, score)
        summary_text = str(row[3]).lower()
        scraped_at = str(row[4])
        score = row[7] if (len(row) > 7 and row[7] is not None) else 0
            
        tags = []
        if score > 0: tags.append('positive')
        elif score < 0: tags.append('negative')
        else: tags.append('neutral')

        if scraped_at.startswith(today_str): tags.append('new_scraped')
        if "giáº£i thá»ƒ" in summary_text: tags.append('priority_keyword')
        elif "cá»• tá»©c" in summary_text: tags.append('co_tuc')

        # ChÃ¨n vÃ o Treeview vÃ  lÆ°u ID dÃ²ng
        item_id = tree_widget.insert('', 'end', values=row[:6], tags=tags)
        
        # LÆ°u ná»™i dung details_clean (index 6) vÃ o item_map vá»›i key lÃ  item_id
        item_map[item_id] = row[6] if len(row) > 6 else "KhÃ´ng cÃ³ chi tiáº¿t."

def update_display(mode="today"):
    global current_view_data, last_count
    if mode == "newly":
        current_view_data = get_newly_scraped_data()
        title_prefix = "Má»›i cáº­p nháº­t"
    elif mode == "5days":
        # ThÃªm logic lá»c 5 ngÃ y á»Ÿ Ä‘Ã¢y
        current_view_data = get_filtered_data(days_offset=5)
        title_prefix = "5 ngÃ y gáº§n nháº¥t"
    else:
        days = 1 if mode == "today" else 7
        current_view_data = get_filtered_data(days_offset=days)
        title_prefix = 'HÃ´m nay' if days==1 else '7 ngÃ y qua'
        
    update_treeview(tree, current_view_data)
    root.title(f"Stock Scraper - {title_prefix} ({len(current_view_data)})")

def on_item_select(event):
    selected = tree.focus()
    if not selected: return
    
    # Láº¥y ná»™i dung tá»« item_map Ä‘Ã£ lÆ°u lÃºc update_treeview
    content = item_map.get(selected, "KhÃ´ng cÃ³ chi tiáº¿t.")
    
    detail_box.config(state=tk.NORMAL)
    detail_box.delete('1.0', tk.END)
    detail_box.insert(tk.END, content)
    highlight_urls(detail_box)
    detail_box.config(state=tk.DISABLED)

def on_combo_confirm(event=None):
    user_input = combo.get().strip().lower() # Chuyá»ƒn vá» chá»¯ thÆ°á»ng vÃ¬ tÃªn table lÃ  chá»¯ thÆ°á»ng
    if not user_input: return
    
    # Danh sÃ¡ch cÃ¡c báº£ng thá»±c táº¿ (Ä‘Ã£ cÃ³ tiá»n tá»‘ event_)
    # 1. Náº¿u ngÆ°á»i dÃ¹ng gÃµ tháº³ng 'event_yeg'
    if user_input in ALL_SPIDERS:
        fetch_history_data(user_input)
        return
        
    # 2. Náº¿u ngÆ°á»i dÃ¹ng chá»‰ gÃµ 'yeg', ta thá»­ tÃ¬m 'event_yeg'
    suggested_table = f"event_{user_input}"
    if suggested_table in ALL_SPIDERS:
        combo.set(suggested_table) # Cáº­p nháº­t láº¡i tÃªn Ä‘áº§y Ä‘á»§ vÃ o combobox cho Ä‘áº¹p
        fetch_history_data(suggested_table)
        return
    
    # 3. Náº¿u váº«n khÃ´ng tháº¥y, thÃ´ng bÃ¡o cho ngÆ°á»i dÃ¹ng
    messagebox.showwarning("KhÃ´ng tÃ¬m tháº¥y", f"KhÃ´ng tÃ¬m tháº¥y báº£ng dá»¯ liá»‡u nÃ o liÃªn quan Ä‘áº¿n '{user_input}'")

def run_auto_script():
    script_path = os.path.join(BASE_DIR, 'auto_run.py')
    if os.path.exists(script_path):
        subprocess.Popen([sys.executable, script_path], cwd=BASE_DIR, shell=False)
        messagebox.showinfo("ThÃ´ng bÃ¡o", "ÄÃ£ kÃ­ch hoáº¡t cháº¿ Ä‘á»™ Auto Run.")

# --- KHá»I Táº O GUI ---
root = tk.Tk()
root.title("Stock Scraper Pro")
root.geometry("1200x850")

main_frame = ttk.Frame(root, padding="15")
main_frame.pack(fill='both', expand=True)

# Top Bar
top_frame = ttk.LabelFrame(main_frame, text="ğŸ” CÃ´ng cá»¥ lá»c nhanh", padding="10")
top_frame.pack(fill='x', pady=(0, 10))

ttk.Label(top_frame, text="MÃ£ CP:").pack(side='left', padx=2)
search_var = tk.StringVar()
search_entry = ttk.Entry(top_frame, textvariable=search_var, width=12)
search_entry.pack(side='left', padx=5)
search_entry.bind('<Return>', lambda e: perform_search())
ttk.Button(top_frame, text="TÃ¬m", command=perform_search).pack(side='left', padx=2)
ttk.Button(top_frame, text="ğŸ“… HÃ´m nay", command=lambda: update_display("today")).pack(side='left', padx=2)

ttk.Button(top_frame, text="âš¡ Má»›i cáº­p nháº­t", command=lambda: update_display("newly")).pack(side='left', padx=2)
ttk.Button(top_frame, text="ğŸ“… 5 NgÃ y", command=lambda: update_display("5days")).pack(side='left', padx=2)
ttk.Button(top_frame, text="âš–ï¸ Luáº­t Viá»‡t Nam", command=show_luatvietnam_only).pack(side='left', padx=5) # NÃšT Má»šI 1
ttk.Label(top_frame, text=" | Nguá»“n:").pack(side='left', padx=5)
combo = ttk.Combobox(top_frame, values=ALL_SPIDERS, state='normal', width=22)
combo.pack(side='left', padx=2)
combo.bind('<<ComboboxSelected>>', on_combo_confirm)
combo.bind('<Return>', on_combo_confirm) # ThÃªm dÃ²ng nÃ y Ä‘á»ƒ nháº­n lá»‡nh khi nháº¥n Enter
# Table
tree = ttk.Treeview(main_frame, columns=('ID', 'MÃ£ CP', 'NgÃ y SK', 'TÃ³m táº¯t', 'Scrape lÃºc', 'Nguá»“n'), show='headings', height=15)
for c in tree['columns']:
    tree.heading(c, text=c, anchor='w')
    tree.column(c, width=100)
tree.column('TÃ³m táº¯t', width=450)
tree.tag_configure('neutral', background='#ffffff')
tree.tag_configure('new_scraped', background='#E8F5E9') # MÃ u xanh lÃ¡ cá»±c nháº¡t cho tin má»›i
tree.tag_configure('co_tuc', foreground='#0000FF')

# Äá»‹nh nghÄ©a cÃ¡c tag quan trá»ng (AI) SAU CÃ™NG Ä‘á»ƒ nÃ³ Ä‘Ã¨ lÃªn mÃ u tin má»›i
tree.tag_configure('positive', background='#e6ffed', foreground='#006400') # Xanh lÃ¡
tree.tag_configure('negative', background='#fff1f0', foreground='#8b0000') # Äá» nháº¡t
tree.tag_configure('priority_keyword', background='#FFF9C4', font=('', 9, 'bold'))

tree.pack(fill='x', pady=5)

# Control Box
ctrl_frame = ttk.LabelFrame(main_frame, text="âš™ï¸ Há»‡ thá»‘ng Scraper", padding="10")
ctrl_frame.pack(fill='x', pady=5)

ttk.Label(ctrl_frame, text="Sá»‘ luá»“ng:").pack(side='left', padx=5)
worker_combo = ttk.Combobox(ctrl_frame, values=["1", "3", "5", "10"], state='readonly', width=5)
worker_combo.set("3")
worker_combo.pack(side='left', padx=5)

progress = ttk.Progressbar(ctrl_frame, length=200, mode='determinate')
progress.pack(side='left', padx=15)

run_btn = ttk.Button(ctrl_frame, text="ğŸš€ Cháº¡y Scrapers", command=lambda: [run_btn.config(state=tk.DISABLED), threading.Thread(target=run_parallel_logic, args=(progress, run_btn), daemon=True).start()])
run_btn.pack(side='left', padx=5)
# NÃšT AUTO RUN Má»šI THÃŠM VÃ€O ÄÃ‚Y
auto_btn = ttk.Button(ctrl_frame, text="ğŸ¤– Auto Run", command=run_auto_script)
auto_btn.pack(side='left', padx=5)

ai_btn = ttk.Button(ctrl_frame, text="âœ¨ PhÃ¢n tÃ­ch AI", command=trigger_ai_analysis)
ai_btn.pack(side='left', padx=5)
impact_btn = ttk.Button(ctrl_frame, text="ğŸ“ˆ ÄÃ¡nh giÃ¡ TTCK", command=run_auto_impact_assessment) # NÃšT Má»šI 2
impact_btn.pack(side='left', padx=5)
# Detail Box
ttk.Label(main_frame, text="Ná»™i dung chi tiáº¿t báº£n tin:", font=('', 9, 'bold')).pack(anchor='w', pady=(10, 0))
detail_box = tk.Text(main_frame, height=30, state=tk.DISABLED, wrap=tk.WORD, bg='#FFFFFF', padx=15, pady=15, font=('Segoe UI', 10))
detail_box.pack(fill='both', expand=True)

tree.bind('<<TreeviewSelect>>', on_item_select)

if __name__ == "__main__":
    update_display("today")
    root.after(300000, auto_refresh) 
    root.mainloop()